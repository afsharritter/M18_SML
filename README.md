# Overview

Machine learning models have proven invaluable in the loan and credit industry, accelerating the rate at which credit applications can be assessed. A process that once took ages to complete, various algorithms now can be used to predict whether a loan application will be low- or high-risk with a high degree of accuracy. Herein, supervised machine learning models like Logistic Regression or Decision Trees aim to predict a credit application's positive or negative status by assessing input data like loan amount, credit history, and demographic information. In this analysis, six machine learning models were tested against a large dataset of credit card applications to predict credit risk.

The dataset of 68,817 rows and 86 columns of loan application data was then separated into the loan application's *input features* like loan amount, credit history, and demographic information from the *target variable,* which classifies the loan application's status as either low- or high-risk. This dataset was then used to train various supervised machine learning models to predict credit risk based on a loan application's input features. Preliminary analysis of the dataset shows a total of 68,817 rows spread over 86 columns of variable datatypes. After separating the *input features* from the *target variable,* it was noted that a considerable imbalance exists between the value counts of each binary outcome in the *target variable* (low-risk [0]: 68,470 | high-risk [1]: 347).

Logistic Regression models predict a binary classification based on input variables. These model types use a sigmoid curve to calculate the probability that a data point will fall within one category or another. While a Logistic Regression model would theoretically do a good job of classifying a balanced loan application dataset as either low- or high-risk, unbalanced datasets cause undue bias in the machine learning model toward the majority class. To overcome this limitation, various Over- and Under-Sampling techniques can be applied to better train the Logistic Regression model. Over-Sampling, like its namesake, involves oversampling the minority class until both classes are even before training the Logistic Regression model. Under-Sampling, on the other hand, involves minimizing the sample size of the majority class before further analysis. A third option, Combination Over- and Under-Sampling, produces a balanced sample dataset that evades the potential noise of Over-Sampling and data loss of Under-Sampling methods. Due to the credit application dataset's large size and *target variable* imbalance, a variety of Over-/Under-Sampling methods will be necessary before Logistic Regression models can predict the credit application's binary outcome as low- or high-risk.

Like the other supervised machine learning model described above, Decision Trees are trained using input data to predict a data point's binary classification or target variable. These models work by deriving a series of conditions from the input features, which are then used to determine a test point's binary outcome. Decision Tree models are relatively impervious to the linear features of a dataset and make a good candidate for testing with credit application data.

# Results

The machine learning algorithms tested in this analysis include two Over-Sampling Models (RandomOverSampler, SMOTE), an Under-Sampling model (ClusterCentroids), an Over- and Under-Sampling model (SMOTEENN), and two Ensemble Classifiers (BalancedRandomForestClassifier, EasyEnsembleClassifier).

# Summary


# References
